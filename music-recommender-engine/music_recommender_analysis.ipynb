{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f616d8",
   "metadata": {},
   "source": [
    "# Project Goal\n",
    "The goal of this project is to build a simple content-based music recommendation engine. The system will recommend songs to a user based on the lyrical similarity of a song they choose.\n",
    "\n",
    "### 1. Data Loading\n",
    "\n",
    "I am using a pre-processed version of the Spotify Million Song Dataset. With a file size of only 75 MB, the dataset is small enough to be loaded directly into a single Pandas DataFrame. This allows for more straightforward data manipulation and analysis, as opposed to processing in chunks.\n",
    "\n",
    "The dataset contains the following key columns:\n",
    "\n",
    "Artist: The artist's name.\n",
    "\n",
    "Song: The song's title.\n",
    "\n",
    "Link: A link to the song's lyrics page.\n",
    "\n",
    "Text: The lyrics of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f0f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your dataset file.\n",
    "file_path = r'.\\spotify_millsongdata.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af5fdc",
   "metadata": {},
   "source": [
    "#### Skipped processing in chunks as dataset is small\n",
    "\n",
    "My initial plan was to process this data in chunks to handle a potentially large file. However, after inspection, the dataset size was found to be much smaller than anticipated (~75MB), making chunked processing unnecessary.\n",
    "\n",
    "The following code was originally planned but skipped:\n",
    "\n",
    "##### # Set the chunk size (e.g., 10,000 rows at a time).\n",
    "chunk_size = 10000\n",
    "\n",
    "##### # Create a generator that reads the file in chunks.\n",
    "chunks = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "##### # You can iterate through the chunks to inspect the data.\n",
    "##### # For now, let's just look at the first chunk to see the column names.\n",
    "first_chunk = next(chunks)\n",
    "print(first_chunk.info())\n",
    "print(first_chunk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire dataset into a single DataFrame.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(df.info())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d36527",
   "metadata": {},
   "source": [
    "### 6.1. Data Sampling: The Solution to MemoryError (Skip this step for now and follow order of numbering)\n",
    "\n",
    "Initial attempts to compute the song similarity matrix resulted in a MemoryError, as the resulting dense matrix was too large to fit into RAM. To solve this, I used data sampling, a common professional strategy for working with large datasets.\n",
    "\n",
    "I took a random sample of 5,000 songs from the original dataset. This approach allows me to build a fully functional and representative proof-of-concept without sacrificing the core methodology, which is a key skill for data professionals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740fcf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the DataFrame to a smaller size (e.g., 5000 songs)\n",
    "df = df.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Print the new shape of the DataFrame\n",
    "print(\"Shape of the sampled DataFrame:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c97502",
   "metadata": {},
   "source": [
    "### 2. Initial Data Exploration\n",
    "After loading the dataset, I'll perform an initial check to understand its structure, identify any missing values, and verify the data types of each column. This is a critical step to ensure the data is clean and ready for analysis.\n",
    "\n",
    "I'll use the following Pandas methods for this exploration:\n",
    "\n",
    "df.info(): Provides a summary of the DataFrame, including the column names, number of non-null values, and data types.\n",
    "\n",
    "df.head(): Displays the first few rows of the DataFrame, giving a quick look at the data.\n",
    "\n",
    "df.isnull().sum(): Counts the number of missing values in each column, which is essential for data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b797e92",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning: No Missing Values\n",
    "\n",
    "After loading the dataset, I performed an initial check for missing values using `df.isnull().sum()`. The results show that there are no missing values in any of the columns. This means the dataset is already clean and ready for analysis, and no further cleaning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34ccbb",
   "metadata": {},
   "source": [
    "### 4. Text Preprocessing: Getting the Lyrics Ready for Analysis\n",
    "\n",
    "To prepare the lyrical data for the recommendation engine, I created a preprocessing function to transform the raw text into a clean and consistent format. This is a crucial step in Natural Language Processing (NLP).\n",
    "\n",
    "The `preprocess_text` function performs the following steps:\n",
    "- **Lowercase Conversion**: All text is converted to lowercase to ensure consistency (`'The'` and `'the'` are treated as the same word).\n",
    "- **Punctuation Removal**: str.maketrans and string.punctuation from string module are used to remove punctuation and special characters that don't contribute to the meaning of the lyrics.\n",
    "- **Tokenization**: The cleaned text is split into individual words.\n",
    "- **Stop Word Removal**: Common, non-meaningful words (e.g., `'a'`, `'the'`, `'is'`) are filtered out using `nltk`'s built-in stop words list.\n",
    "- **Stemming**: I used the `PorterStemmer` to reduce words to their root form (e.g., `'running'` and `'runs'` become `'run'`). This helps improve the accuracy of the similarity calculation.\n",
    "\n",
    "I applied this function to the `text` column of the DataFrame to create a new `processed_text` column, which now contains the cleaned and ready-to-use lyrical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98060b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af9438-646b-4c1c-aece-f11668018920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess function\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses a string of text for natural language processing (NLP).\n",
    "\n",
    "    The function performs a series of operations to prepare the text:\n",
    "    1. Converts text to lowercase.\n",
    "    2. Removes all punctuation.\n",
    "    3. Tokenizes the text into a list of words.\n",
    "    4. Removes common English stop words.\n",
    "    5. Applies stemming to reduce words to their root form.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text string to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text as a single string of stemmed words.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "\n",
    "    # Create a translation table to delete all punctuation characters\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    # Apply the translation table to the text\n",
    "    text_without_punctuation = text.translate(translator)\n",
    "\n",
    "    # Initialize stop words and stemmer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text_without_punctuation)\n",
    "    \n",
    "    # Process tokens in a single loop (list comprehension)\n",
    "    processed_tokens = [\n",
    "        stemmer.stem(word) for word in tokens if word not in stop_words\n",
    "    ]\n",
    "\n",
    "    # Join the processed tokens back into a string (optional)\n",
    "    return \" \".join(processed_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0548a6e4-5fe8-40c8-8aeb-16c4f69a2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'processed_text' by applying your function to the 'text' column\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# You can now view the original text side-by-side with the processed text\n",
    "print(df[['text', 'processed_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d69904",
   "metadata": {},
   "source": [
    "### 5. Vectorization: Converting Text to Numbers\n",
    "\n",
    "The next step was to convert the pre-processed lyrical data into a numerical format. I used **TF-IDF (Term Frequency-Inverse Document Frequency)**, a statistical method that reflects how important a word is to a song within the entire dataset.\n",
    "\n",
    "While other methods like Bag-of-Words and Word Embeddings exist, TF-IDF was chosen for this project because it effectively accounts for word importance, which is crucial for a content-based recommendation engine. It provides a strong balance of simplicity and accuracy for our needs.\n",
    "\n",
    "- **`TfidfVectorizer`**: I used scikit-learn's `TfidfVectorizer` to perform this conversion. This tool is highly efficient and handles all the steps—from tokenization to calculating the TF-IDF scores—in a single, optimized operation.\n",
    "- **`tfidf_matrix`**: The result of this process is a sparse matrix, where each row represents a song and each column represents a unique word. The values in the matrix are the TF-IDF scores for each word, which we will use to calculate song similarity.\n",
    "\n",
    "The dimensions of the resulting matrix are (Number of Songs, Number of Unique Words), confirming that our text data has been successfully vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d08b9-af16-46fd-8002-33bfe64c68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "# The TfidfVectorizer handles tokenization, counting, and TF-IDF calculation\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the processed text to create the TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# Print the shape of the matrix to see its dimensions\n",
    "print(\"Shape of TF-IDF matrix:\", tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812c66a",
   "metadata": {},
   "source": [
    "### 6. Calculating Song Similarity\n",
    "\n",
    "With the lyrical data now in a numerical format, the next step was to measure the similarity between each song. I used **Cosine Similarity**, a metric that calculates the cosine of the angle between two TF-IDF vectors. The resulting score, which ranges from 0 to 1, indicates how similar two songs are in their lyrical content.\n",
    "\n",
    "- **`cosine_similarity()`**: I used scikit-learn's `cosine_similarity` function to compute this metric on our `tfidf_matrix`.\n",
    "- **`cosine_sim`**: The output is a square matrix where each cell represents the similarity score between two songs. For example, `cosine_sim[0][1]` holds the similarity score between the first and second songs in our dataset.\n",
    "\n",
    "This matrix is the core of the recommendation engine, as it provides the foundation for finding and recommending songs similar to a user's selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1550c42",
   "metadata": {},
   "source": [
    "#### (Initial Attempt)\n",
    "\n",
    "An initial attempt to compute the cosine similarity matrix on the full dataset resulted in a MemoryError. The error occurred because the output matrix, which is dense and needs to store a similarity score for every possible pair of songs, was too large to fit in my computer's RAM.\n",
    "\n",
    "The tfidf_matrix, which is a sparse representation of the data, was small enough, but the cosine_similarity() function from scikit-learn attempted to create a dense matrix of shape (57650, 57650) with over 3.3 billion elements, requiring 23.9 GB of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16602668",
   "metadata": {},
   "source": [
    "### 6.1 Solution: Data Sampling (Now move above to just after step 1)\n",
    "To solve this, I chose a common professional strategy: data sampling. By taking a random sample of 5,000 songs from the original dataset, I was able to reduce the size of the TF-IDF matrix, allowing the final cosine similarity matrix to be computed without any memory issues.  This approach allows for the creation of a fully functional and representative proof-of-concept, demonstrating a practical solution to a common data science challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5eb5e6",
   "metadata": {},
   "source": [
    "### 6.2 Calculating Song Similarity (Successful Attempt)\n",
    "After sampling the data, the process was successful. The code below computes the cosine similarity matrix on the now smaller tfidf_matrix. The resulting matrix is the core of our recommendation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae93ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Print the shape of the matrix to see its dimensions\n",
    "print(\"Shape of cosine similarity matrix:\", cosine_sim.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddadb6b",
   "metadata": {},
   "source": [
    "### 7. Building the Recommendation Engine\n",
    "\n",
    "With the cosine similarity matrix computed, the final step was to build the core recommendation function. The `get_recommendations` function takes a song title as input and performs the following tasks:\n",
    "\n",
    "- **Finds the Index**: It uses the song title to locate its corresponding index in the DataFrame.\n",
    "- **Retrieves Scores**: It fetches the similarity scores for that song from the `cosine_sim` matrix.\n",
    "- **Sorts and Filters**: It sorts the scores to find the most similar songs, ensuring the input song itself is not included in the recommendations.\n",
    "- **Returns Recommendations**: It uses the indices of the top-ranked songs to retrieve and display their titles and artists.\n",
    "\n",
    "This final function ties all the previous steps—data cleaning, vectorization, and similarity calculation—together into a complete and functional music recommendation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a921255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_recommendations function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_recommendations(song_title, cosine_sim):\n",
    "    \"\"\"\n",
    "    Generates a list of song recommendations based on lyrical similarity.\n",
    "\n",
    "    The function finds songs with similar lyrical content to a given song\n",
    "    by using a pre-computed cosine similarity matrix.\n",
    "\n",
    "    Args:\n",
    "        song_title (str): The title of the song to get recommendations for.\n",
    "        cosine_sim (np.ndarray): The pre-computed cosine similarity matrix.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of recommended songs, with each song represented as a\n",
    "              Pandas Series containing its information. Returns an empty list\n",
    "              if the song is not found.\n",
    "    \"\"\"\n",
    "    # Find the index of the song that matches the title\n",
    "    # .tolist() is used to convert the Index object to a simple list\n",
    "    song_indices = df.index[df['song'] == song_title].tolist()\n",
    "\n",
    "    # Check if the song was found in the DataFrame\n",
    "    if not song_indices:\n",
    "        print(f\"Song '{song_title}' not found in the dataset.\")\n",
    "        return []\n",
    "\n",
    "    # Get the similarity scores for the chosen song from the cosine similarity matrix\n",
    "    # [0] is used to get the single index from the list\n",
    "    sim_scores = cosine_sim[song_indices[0]]\n",
    "\n",
    "    # Get the indices of the songs sorted by similarity score in descending order\n",
    "    # np.argsort returns the indices that would sort the array\n",
    "    # reversed() is used to get them from most to least similar\n",
    "    sorted_indices = np.argsort(sim_scores)\n",
    "    \n",
    "    # Use a list comprehension to filter out the input song's own index\n",
    "    # The list comprehension is a more efficient and \"Pythonic\" way to do this\n",
    "    rec_indices = [\n",
    "        index for index in reversed(sorted_indices)\n",
    "        if index != song_indices[0]\n",
    "    ]\n",
    "\n",
    "    # Take the top 10 recommendations from the filtered list\n",
    "    top_10_rec = rec_indices[:10]\n",
    "\n",
    "    # Use a list comprehension to retrieve the actual songs from the DataFrame\n",
    "    # df.iloc[i] is used to get the entire row (song) by its integer index\n",
    "    recommended_songs = [\n",
    "        df.iloc[i] for i in top_10_rec\n",
    "    ]\n",
    "\n",
    "    return recommended_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0237d1d4",
   "metadata": {},
   "source": [
    "### 8. The Recommendation Interface\n",
    "\n",
    "To make the recommendation engine interactive and user-friendly, I created a `main` function that serves as a simple command-line interface. This function guides the user to input a song title and then displays the top 10 recommended songs based on the model's output.\n",
    "\n",
    "The `main` function handles the following:\n",
    "- Prompts the user for a song title.\n",
    "- Calls the `get_recommendations` function, which contains all the core logic.\n",
    "- Displays the results in a clean, readable list, showing the song title and artist for each recommendation.\n",
    "\n",
    "This interactive interface demonstrates the full functionality of the project, from raw data to a practical application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. Get user input for a song title\n",
    "    print(\"Enter a song title to get recommendations.\")\n",
    "    print(\"Note: The model was trained on a sample of 5000 songs, so not all songs are available.\")\n",
    "    print(\"\\nFor a successful test, try titles after checking with the dataframe df.\")\n",
    "    song_title = input(\"\\nPlease Enter Song Title: \")\n",
    "\n",
    "    # Use a try-except block to handle cases where the song is not in the DataFrame\n",
    "    try:\n",
    "        artist_name = df.loc[df['song'] == song_title, 'artist'].item()\n",
    "        print(f\"\\nYou entered the song '{song_title}' by {artist_name}\")\n",
    "    except (KeyError, IndexError):\n",
    "        print(f\"\\nSong '{song_title}' not found in the dataset.\")\n",
    "        return\n",
    "\n",
    "    # 2. Call the get_recommendations function\n",
    "    rec_list = get_recommendations(song_title, cosine_sim)\n",
    "\n",
    "    # 3. Print the results in a readable format\n",
    "    print(\"\\nHere are your recommendations:\")\n",
    "    for i, rec_song in enumerate(rec_list, 1):\n",
    "        print(f\"{i}. {rec_song['song']} by {rec_song['artist']}\")\n",
    "    \n",
    "# This is a standard Python practice to run the main function when the script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718030b3",
   "metadata": {},
   "source": [
    "Thank you for your time and for reviewing my project. This music recommendation engine represents the culmination of the skills I've gained in data manipulation, text processing, and machine learning principles. It was a valuable exercise in building a complete data science pipeline from scratch, and I am excited to continue applying these skills to future projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
