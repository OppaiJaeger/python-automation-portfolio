{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f616d8",
   "metadata": {},
   "source": [
    "# Project Goal\n",
    "The goal of this project is to build a simple content-based music recommendation engine. The system will recommend songs to a user based on the lyrical similarity of a song they choose.\n",
    "\n",
    "### 1. Data Loading\n",
    "\n",
    "I am using a pre-processed version of the Spotify Million Song Dataset. With a file size of only 75 MB, the dataset is small enough to be loaded directly into a single Pandas DataFrame. This allows for more straightforward data manipulation and analysis, as opposed to processing in chunks.\n",
    "\n",
    "The dataset contains the following key columns:\n",
    "\n",
    "Artist: The artist's name.\n",
    "\n",
    "Song: The song's title.\n",
    "\n",
    "Link: A link to the song's lyrics page.\n",
    "\n",
    "Text: The lyrics of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f0f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your dataset file.\n",
    "file_path = r'.\\spotify_millsongdata.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af5fdc",
   "metadata": {},
   "source": [
    "#### Skipped processing in chunks as dataset is small\n",
    "\n",
    "// Set the chunk size (e.g., 10,000 rows at a time).\n",
    "\n",
    "chunk_size = 10000\n",
    "\n",
    "// Create a generator that reads the file in chunks.\n",
    "\n",
    "chunks = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "// You can iterate through the chunks to inspect the data.\n",
    "\n",
    "// For now, let's just look at the first chunk to see the column names.\n",
    "\n",
    "first_chunk = next(chunks)\n",
    "\n",
    "print(first_chunk.info())\n",
    "\n",
    "print(first_chunk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire dataset into a single DataFrame.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(df.info())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c97502",
   "metadata": {},
   "source": [
    "### 2. Initial Data Exploration\n",
    "After loading the dataset, I'll perform an initial check to understand its structure, identify any missing values, and verify the data types of each column. This is a critical step to ensure the data is clean and ready for analysis.\n",
    "\n",
    "I'll use the following Pandas methods for this exploration:\n",
    "\n",
    "df.info(): Provides a summary of the DataFrame, including the column names, number of non-null values, and data types.\n",
    "\n",
    "df.head(): Displays the first few rows of the DataFrame, giving a quick look at the data.\n",
    "\n",
    "df.isnull().sum(): Counts the number of missing values in each column, which is essential for data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b797e92",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning: No Missing Values\n",
    "\n",
    "After loading the dataset, I performed an initial check for missing values using `df.isnull().sum()`. The results show that there are no missing values in any of the columns. This means the dataset is already clean and ready for analysis, and no further cleaning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34ccbb",
   "metadata": {},
   "source": [
    "### 4. Text Preprocessing: Getting the Lyrics Ready for Analysis\n",
    "\n",
    "To prepare the lyrical data for the recommendation engine, I created a preprocessing function to transform the raw text into a clean and consistent format. This is a crucial step in Natural Language Processing (NLP).\n",
    "\n",
    "The `preprocess_text` function performs the following steps:\n",
    "- **Lowercase Conversion**: All text is converted to lowercase to ensure consistency (`'The'` and `'the'` are treated as the same word).\n",
    "- **Punctuation Removal**: Regular expressions are used to remove punctuation and special characters that don't contribute to the meaning of the lyrics.\n",
    "- **Tokenization**: The cleaned text is split into individual words.\n",
    "- **Stop Word Removal**: Common, non-meaningful words (e.g., `'a'`, `'the'`, `'is'`) are filtered out using `nltk`'s built-in stop words list.\n",
    "- **Stemming**: I used the `PorterStemmer` to reduce words to their root form (e.g., `'running'` and `'runs'` become `'run'`). This helps improve the accuracy of the similarity calculation.\n",
    "\n",
    "I applied this function to the `text` column of the DataFrame to create a new `processed_text` column, which now contains the cleaned and ready-to-use lyrical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98060b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af9438-646b-4c1c-aece-f11668018920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess function\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses a string of text for natural language processing (NLP).\n",
    "\n",
    "    The function performs a series of operations to prepare the text:\n",
    "    1. Converts text to lowercase.\n",
    "    2. Removes all punctuation.\n",
    "    3. Tokenizes the text into a list of words.\n",
    "    4. Removes common English stop words.\n",
    "    5. Applies stemming to reduce words to their root form.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text string to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text as a single string of stemmed words.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # Create a translation table to delete all punctuation characters\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # Apply the translation table to the text\n",
    "    text_without_punctuation = text.translate(translator)\n",
    "    # Initialize stop words and stemmer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text_without_punctuation)\n",
    "    # Process tokens in a single loop (list comprehension)\n",
    "    processed_tokens = [\n",
    "        stemmer.stem(word) for word in tokens if word.lower() not in stop_words\n",
    "    ]\n",
    "\n",
    "    # Join the processed tokens back into a string (optional)\n",
    "    return \" \".join(processed_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0548a6e4-5fe8-40c8-8aeb-16c4f69a2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'processed_text' by applying your function to the 'text' column\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# You can now view the original text side-by-side with the processed text\n",
    "print(df[['text', 'processed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d08b9-af16-46fd-8002-33bfe64c68c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
